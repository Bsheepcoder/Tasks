#Python网络爬虫与信息提取笔记

掌握定向网络数据爬取和网页解析的基本能力

the website is the API



[toc]

##1.Python开发工具选择

IDE集成开发环境

<a href="https://imgchr.com/i/yFxQbR"><img src="https://s3.ax1x.com/2021/01/30/yFxQbR.png" alt="yFxQbR.png" border="0" /></a>



本课需要IDLE、PyCharm、Sublime Text、Anaconda & Spyder



IDLE python自带

Sublime Text 专为程序员

wing 公司维护工具收费，多人开发，版本同步

Visual Studio & PTVS  微软公司维护 win环境为主 调试丰富

Eclipse 通过PyDev配置 开源IDE开发工具

PyCharm 社区免费、简单、集成度高



科学计算领域

Canopy 公司维护，工具收费、支持500个第三方库

Anaconda 开源免费，更多第三方库



##2.Requests库入门

###0x1 requests的安装方法

www.python-requestes.org

```cmd
pip install requsets
```

测试

```python
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> r.status_code
200
>>> r.encoding = 'utf-8'
>>> r.text

输出
百度web页面的源码
```

安装正常



### 0x2 主要函数方法

<a href="https://imgchr.com/i/yk9gkn"><img src="https://s3.ax1x.com/2021/01/30/yk9gkn.png" alt="yk9gkn.png" border="0" /></a>



#### get方法

```python
r = requests.get(url)
```

构造一个向服务器请求资源的Request对象

返回一个包含服务器资源的Response对象

大小写敏感

```python
requests.get(url,params=None,**kwargs)
```

url 获取页面连接

params： url中的额外参数，字典或字节流格式，可选

**kwargs：12个控制访问的参数

<a href="https://imgchr.com/i/ykuE34"><img src="https://s3.ax1x.com/2021/01/30/ykuE34.png" alt="ykuE34.png" border="0" /></a>



####get包含两个对象

Request对象

Response对象 



####Response对象

```python
>>> import requests
>>> r = requests.get("http://www.baidu.com")  #访问网址
>>> print(r.status_code)  #请求页面的状态码
200   #状态码为200则访问成功，不是则失败
>>> type(r)  #检查r的类型
<class 'requests.models.Response'>  #输出了类名，是response
>>> r.headers
{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Sat, 30 Jan 2021 08:10:06 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:36 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}
```

Response对象属性

<a href="https://imgchr.com/i/yktqUJ"><img src="https://s3.ax1x.com/2021/01/30/yktqUJ.png" alt="yktqUJ.png" border="0" />



####获取网页流程

先检查状态，如果是200则可以解析

如果状态码是其他则，访问错误



####实例运用

```python
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> print(r.status_code)
200
>>> r.text
'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç\x99¾åº¦ä¸\x80ä¸\x8bï¼\x8cä½\xa0å°±ç\x9f¥é\x81\x93</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=ç\x99¾åº¦ä¸\x80ä¸\x8b class="bg s_btn"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ\x96°é\x97»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å\x9c°å\x9b¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§\x86é¢\x91</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å\x90§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç\x99»å½\x95</a> </noscript> <script>document.write(\'<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ \'" name="tj_login" class="lb">ç\x99»å½\x95</a>\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">æ\x9b´å¤\x9aäº§å\x93\x81</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å\x85³äº\x8eç\x99¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç\x94¨ç\x99¾åº¦å\x89\x8då¿\x85è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æ\x84\x8fè§\x81å\x8f\x8dé¦\x88</a>&nbsp;äº¬ICPè¯\x81030173å\x8f·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n'
>>> r.encoding
'ISO-8859-1'
>>> r.apparent_encoding
'utf-8'
>>> r.encoding = 'utf-8'
>>> r.text
'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=百度一下 class="bg s_btn"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write(\'<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ \'" name="tj_login" class="lb">登录</a>\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n'
>>> 
```

- 根据实例，如果网页服务器中含有charset字段，即说明服务器对自身资源编码有要求，访问时会获得该字段，并顺利解析出人类可读的内容

- 但如果没有charset字段，则会默认编码为'ISO-8859-1'，该编码不能解析中文，则需要用apparent_encoding分析其可解析为中文的编码方式。encoding只是在提取内容的编码，只有将apparent_encoding赋给encoding，才能获得中文可读的资源 



####理解Requests库的异常

<a href="https://imgchr.com/i/ykz4Kg"><img src="https://s3.ax1x.com/2021/01/30/ykz4Kg.png" alt="ykz4Kg.png" border="0" /></a>



判断http状态是否异常，异常返回Error

<a href="https://imgchr.com/i/yAPUXT"><img src="https://s3.ax1x.com/2021/01/30/yAPUXT.png" alt="yAPUXT.png" border="0" /></a>



####爬取网页的通用代码框架

使爬取网页更稳定有效

```python
import requests
def getHTMLText(url):
    try:
        r = requests.get(url,timeout=30)
        r.raise_for_status() #如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding  #使解码正确
        return r.text
    except:
        return"产生异常"
if _name_ == "_main_":
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```



####HTTP协议

超文本传输协议

HTTP是一个基于“请求与响应”模式的、无状态的应用层协议

采用URL格式 

```python
http：//host[:port][path]
```

host: 合法的Internet主机域名或IP地址

post：端口号，缺省端口为80

path：请求资源的路径



#####HTTP URL的理解：

URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源



####HTTP协议对资源的操作

<a href="https://imgchr.com/i/yAn4Mj"><img src="https://s3.ax1x.com/2021/01/30/yAn4Mj.png" alt="yAn4Mj.png" border="0" /></a>

<a href="https://imgchr.com/i/yAnxy9"><img src="https://s3.ax1x.com/2021/01/30/yAnxy9.png" alt="yAnxy9.png" border="0" /></a>

#####理解PATCH和PUT的区别

假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段。

需求：用户修改了UserName，其他不变

采用PATCH，仅向URL提交UserName的局部更新请求

采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除

PATCH节省网络带宽



####HTTP协议与Requests库的关系

是一一对应的

<a href="https://imgchr.com/i/yAuhtK"><img src="https://s3.ax1x.com/2021/01/31/yAuhtK.png" alt="yAuhtK.png" border="0" /></a>

requests.head方法

展示头部信息内容，用更少的网络流量

```python
>>> r = requests.head('http://httpbin.org/get')
>>> r.headers
{'Date': 'Sat, 30 Jan 2021 16:06:46 GMT', 'Content-Type': 'application/json', 'Content-Length': '307', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}
>>> r.text
''
```



requests.post方法

如果向URL POST一个字典、键值对，会默认添加到表单form的字段下

```python
>>> payload = {'key1':'value1','key2':'value2'}
>>> r = requests.post('http://httpbin.org/post',data = payload)
>>> print(r.text)
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "key1": "value1", 
    "key2": "value2"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "23", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.25.1", 
    "X-Amzn-Trace-Id": "Root=1-60158558-7cb778f10d734f1b605e2cae"
  }, 
  "json": null, 
  "origin": "121.56.130.130", 
  "url": "http://httpbin.org/post"
}
```

如果不post键值对，而是一个字符串，则会存储到data的字段内

```python
>>> r = requests.post('http://httpbin.org/post',data = 'ABC')
>>> print(r.text)
{
  "args": {}, 
  "data": "ABC", 
  "files": {}, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "3", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.25.1", 
    "X-Amzn-Trace-Id": "Root=1-60158634-3617046c013a64a120efe139"
  }, 
  "json": null, 
  "origin": "121.56.130.130", 
  "url": "http://httpbin.org/post"
}
```

###0x3主要方法的解析

####requests.request

```pyhton
requests.request(method,url,**kwargs)
```

######method:

请求方式，对应get/put/post等7种

<a href="https://imgchr.com/i/yAMtI0"><img src="https://s3.ax1x.com/2021/01/31/yAMtI0.png" alt="yAMtI0.png" border="0" /></a>

######**kwargs：

控制访问的参数，可选项，共13个

<a href="https://imgchr.com/i/yAM7dI"><img src="https://s3.ax1x.com/2021/01/31/yAM7dI.png" alt="yAM7dI.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQFYV"><img src="https://s3.ax1x.com/2021/01/31/yAQFYV.png" alt="yAQFYV.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQeOJ"><img src="https://s3.ax1x.com/2021/01/31/yAQeOJ.png" alt="yAQeOJ.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQKT1"><img src="https://s3.ax1x.com/2021/01/31/yAQKT1.png" alt="yAQKT1.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQlY6"><img src="https://s3.ax1x.com/2021/01/31/yAQlY6.png" alt="yAQlY6.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQ8SO"><img src="https://s3.ax1x.com/2021/01/31/yAQ8SO.png" alt="yAQ8SO.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQJ6e"><img src="https://s3.ax1x.com/2021/01/31/yAQJ6e.png" alt="yAQJ6e.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQYOH"><img src="https://s3.ax1x.com/2021/01/31/yAQYOH.png" alt="yAQYOH.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQaTI"><img src="https://s3.ax1x.com/2021/01/31/yAQaTI.png" alt="yAQaTI.png" border="0" /></a>

url：获取页面的url链接

####requests.get

```python
requests.get(url,params=None,**kwargs)
```

url: 你获取页面的url链接

params：url中的额外参数，字典或字节流格式，可选

**kwargs：12个控制访问的参数，同上

####requests.head

```python
requests.head(url,**kwargs)
```

url: 你获取页面的url链接

**kwargs：13个控制访问的参数，同上

####requests.post

```python
requests.post(url,data=None.json=None,**kwargs)
```

url:拟更新页面的url链接

data：字典、字节序列或文件，Request的内容

json：JSON格式的数据，Request的内容

**kwargs：11个访问的参数，同上

####requests.put

```python
requests.put(url,data=None,**kwargs)
```

url:拟更新页面的url链接

data：字典、字节序列或文件，Request的内容

**kwargs：12个访问的参数，同上

####requests.patch

```python
requests.patch(url,data=None,**kwargs)
```

url:拟更新页面的url链接

data：字典、字节序列或文件，Request的内容

**kwargs：12个访问的参数，同上

####requests.delete

```python
requests.delete(url,**kwargs)
```

url:拟更新页面的url链接

**kwargs：13个访问的参数，同上



####爬取100次用时的代码

```python
def getHtmlText(url):
   try:  
       r = requests.get(url, timeout=30)  
       r.raise_for_status() 
       r.encoding = r.apparent_encoding        
       return r.text   
   except:
       return '运行异常'
if __name__ == "__main__": 
   url = 'https://www.baidu.com' #任意填入某个网址即可，我爬的百度
   totaltime = 0
   for i in range(100):
       starttime = time.perf_counter()
       getHtmlText(url)
       endtime = time.perf_counter()
       print('第{0}次爬取，用时{1:.4f}秒'.format(i+1, endtime-starttime))
       totaltime=totaltime+endtime-starttime       
   print('总共用时{:.4f}秒'.format(totaltime))
```



###0x2 关于http状态码

####1开头的http状态码

表示临时响应并需要请求者继续执行操作的状态代码。

100  （继续） 请求者应当继续提出请求。 服务器返回此代码表示已收到请求的第一部分，正在等待其余部分。 
101  （切换协议） 请求者已要求服务器切换协议，服务器已确认并准备切换。

####2开头的http状态码
表示请求成功

200   成功处理了请求，一般情况下都是返回此状态码； 
201   请求成功并且服务器创建了新的资源。 
202   接受请求但没创建资源； 
203   返回另一资源的请求； 
204   服务器成功处理了请求，但没有返回任何内容；
205   服务器成功处理了请求，但没有返回任何内容；
206   处理部分请求；

#### 3开头的http状态码（重定向） 

重定向代码，也是常见的代码

300  （多种选择） 针对请求，服务器可执行多种操作。 服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 
301  （永久移动） 请求的网页已永久移动到新位置。 服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。 
302  （临时移动） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 
303  （查看其他位置） 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。 
304  （未修改） 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。 
305  （使用代理） 请求者只能使用代理访问请求的网页。 如果服务器返回此响应，还表示请求者应使用代理。 
307  （临时重定向） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。


#### 4开头的http状态码表示请求出错

400   服务器不理解请求的语法。 
401  请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 
403  服务器拒绝请求。 
404  服务器找不到请求的网页。 
405  禁用请求中指定的方法。 
406  无法使用请求的内容特性响应请求的网页。 
407  此状态代码与 401类似，但指定请求者应当授权使用代理。 
408  服务器等候请求时发生超时。 
409  服务器在完成请求时发生冲突。 服务器必须在响应中包含有关冲突的信息。 
410  如果请求的资源已永久删除，服务器就会返回此响应。 
411  服务器不接受不含有效内容长度标头字段的请求。 
412  服务器未满足请求者在请求中设置的其中一个前提条件。 
413  服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 
414  请求的 URI（通常为网址）过长，服务器无法处理。 
415  请求的格式不受请求页面的支持。 
416  如果页面无法提供请求的范围，则服务器会返回此状态代码。 
417  服务器未满足”期望”请求标头字段的要求。

#### 5开头状态码并不常见

但是我们应该知道

500  （服务器内部错误） 服务器遇到错误，无法完成请求。 
501  （尚未实施） 服务器不具备完成请求的功能。 例如，服务器无法识别请求方法时可能会返回此代码。 
502  （错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。 
503  （服务不可用） 服务器目前无法使用（由于超载或停机维护）。 通常，这只是暂时状态。 
504  （网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 
505  （HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。

</a>

##3.robots.txt网络爬虫排除协议

### 0x1 网络爬虫引发的问题

#### 网络爬虫的尺寸

<a href="https://imgchr.com/i/yAlqxS"><img src="https://s3.ax1x.com/2021/01/31/yAlqxS.png" alt="yAlqxS.png" border="0" /></a>



#### 网络爬虫的骚扰

就如骚扰电话

服务器不能提供多次的访问



#### 网络爬虫的法律风险

服务器上的数据又产权归属

网络爬虫获取数据后牟利是非法的



#### 网络爬虫泄露隐私

网络爬虫可能具备突破简单访问控制的能力，

获取被保护数据从而泄露个人隐私



#### 网络爬虫的限制

- 来源审查 判断User-Agent进行限制
  - 检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问

- 发布公告 Robots协议
  - 告知所有爬虫网址的爬取策略，要求爬虫遵守



### 0x2 Robots协议

网络爬虫排除标准

作用：网站告知所有爬虫网址的爬取策略

形式：在网站根目录下的robots.txt文件，写明了哪些网站不能爬取

<a href="https://imgchr.com/i/yA30t1"><img src="https://s3.ax1x.com/2021/01/31/yA30t1.png" alt="yA30t1.png" border="0" /></a>一一般网站都会有

用户名和不能访问的目录



Robots协议的遵守方式

网络爬虫：自动或人工识别robots.txt,再进行内容爬取

约束性：Robots协议是建议但非约束性，网络怕虫可以不遵守，但存在法律风险



类人行为可不参考Robots协议

####具体含义

User-agent: *Allow是robots文件中的bai一句语法，代du表的意思是：允许所有的搜zhi索引擎可dao以按照robots文件中的限制语法进行合zhuan理的抓取网站中shu的文件、目录。

User-agent: *Disallow：是允许所有搜索引擎收录的意思。

User-agent: *表示允许所有搜索引擎蜘道蛛来爬行抓取，也可以把*去掉，改为特定某一个或者某些搜索引擎蜘蛛来爬行抓取，如百度是Baiduspider，谷歌是Googlebot。

1、允许所有SE（搜索引擎）收录本站：robots.txt为空就可以，什么都不要写。

2、禁止所有SE（搜索引擎）收录网站的某些目录：

User-agent: *

Disallow: /目录名1/

Disallow: /目录名2/

Disallow: /目录名3/

3、 禁止某个SE（搜索引擎）收录本站，例如禁止百度：

User-agent: Baiduspider

Disallow: /

4、 禁止所有SE（搜索引擎）收录本站：
User-agent: *

Disallow: /

##4.Requests库实战项目

### 0x1 简单的爬取页面

```python
>>> import requests
>>> r = requests.get("https://item.jd.com/10026061635284.html")
>>> r.status_code
200
>>> r.encoding
'UTF-8'
>>> r.text[:1000]
"<script>window.location.href='https://passport.jd.com/new/login.aspx?ReturnUrl=http%3A%2F%2Fitem.jd.com%2F10026061635284.html'</script>"
```



### 0x2 爬取页面的API被网页识别

```python
>>> import requests
>>> r = requests.get("https://www.amazon.cn/dp/B015WFYLYS/ref=sr_1_1?__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&dchild=1&pf_rd_i=2031278071&pf_rd_m=A1U5RCOVU0NYF2&pf_rd_p=2cb8106c-dc3a-4935-8a33-30ed2973e130&pf_rd_r=NHV88T361TRYD2EK6WWS&pf_rd_s=merchandised-search-top-1&pf_rd_t=101&qid=1612062162&s=jewelry&sr=1-1")
>>> r.status_code
503
>>> r.encoding
'ISO-8859-1'
>>> r.apparent_encoding
'utf-8'
>>> r.encoding = "utf-8"
>>> r.text
输出带警告的网页内容
>>> r.request.headers
{'User-Agent': 'python-requests/2.25.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
>>> kv = {'user-agent':'Mozilla/5.0'}
>>> r = requests.get("https://www.amazon.cn/dp/B015WFYLYS/ref=sr_1_1?__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&dchild=1&pf_rd_i=2031278071&pf_rd_m=A1U5RCOVU0NYF2&pf_rd_p=2cb8106c-dc3a-4935-8a33-30ed2973e130&pf_rd_r=NHV88T361TRYD2EK6WWS&pf_rd_s=merchandised-search-top-1&pf_rd_t=101&qid=1612062162&s=jewelry&sr=1-1",headers = kv)
>>> r.status_code
200
>>> r.encoding
'ISO-8859-1'
>>> r.encoding = r.apparent_encoding
>>> r.text
将会输出不带警告的内容
```

```python
import requests
url : "https://www.amazon.cn/dp/B015WFYLYS/ref=sr_1_1?__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&dchild=1&pf_rd_i=2031278071&pf_rd_m=A1U5RCOVU0NYF2&pf_rd_p=2cb8106c-dc3a-4935-8a33-30ed2973e130&pf_rd_r=NHV88T361TRYD2EK6WWS&pf_rd_s=merchandised-search-top-1&pf_rd_t=101&qid=1612062162&s=jewelry&sr=1-1"
try:
    kv = {'uesr-agent':'Mozilla/5.0'}  #模拟浏览器
    r = requests.get(url,headers=kv)
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    print(r.text[1000:2000])
except:
    print("爬取失败")

```

### 0x3 百度360搜索关键字提交

浏览器一般会提供搜索接口

百度关键词接口：http://www.baidu.com/s?wd=keyword

360的管家词接口：http://www.so.com/s?q=keyword

必应的关键词接口：https://cn.bing.com/search?q=keyword

交互式代码

```python
>>> import requests
>>> kv = {'wd':'python'}
>>> r = requests.get("http://www.baidu.com/s",params = kv)
>>> r.status_code
200
>>> r.request.url
'https://wappass.baidu.com/static/captcha/tuxing.html?&ak=c27bbc89afca0463650ac9bde68ebe06&backurl=https%3A%2F%2Fwww.baidu.com%2Fs%3Fwd%3Dpython&logid=9999058327848974287&signature=f3e935e0353f02548afe879c3d3f4836&timestamp=1612065002'
>>> len(r.text)
1519
>>> 
```

文件式代码

```python
import requests
keyword = "Python"
try:
    kv = {'wd':keyword}
    r = requests.get("http://www.baidu.com/s",params=kv)
    print(r.request.url)
    r.raise_for_status()
    print(len(r.text))
except:
    print("爬取失败")
```

### 0x4 网络图片的爬取和存储

网络图片链接格式

http://www.example.com/picture.jpg

国家地理网站

http://www.nationalgeographic.com.cn/

选择一个图片Web页面

http://www.nationalgeograpuic.com.cn/photography/photo_of_the_day/3921.html

![图片](http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg)

为图片链接

https://img10.360buyimg.com/babel/s590x470_jfs/t1/151211/37/16803/98451/601143c1Eed1241d0/1bd72a656dd91682.jpg.webp

交互代码

```python
>>> import requests
>>> path = "D:/abc.jpg"
>>> url = "https://img10.360buyimg.com/babel/s590x470_jfs/t1/151211/37/16803/98451/601143c1Eed1241d0/1bd72a656dd91682.jpg.webp"
>>> r = requests.get(url)
>>> r.status_code
200
>>> with open(path,'wb') as f:
	f.write(r.content)

	
60464
>>> f.close()
```

文本代码

```python
import requests
import os
url = "https://img10.360buyimg.com/babel/s590x470_jfs/t1/151211/37/16803/98451/601143c1Eed1241d0/1bd72a656dd91682.jpg.webp"
root = "D:/abc.jpg"
path = root +url.spit('/')[-1]
try:
    if not os.path.exists(root):
        os.mkdir(root)
    if not os.path.exists(path):
        r = requests.get(url)
        with open(path,'wb') as f:
            f.write(r.content)
            f.close()
            print("文件保存成功")
    else:
        print("文件保存成功")
except:
    print("爬取失败")
```

==要保证代码的稳定性，即在合理的风险内依然能有效运行==



### 0x5 IP地址查询

[ip地址含义](https://www.cnblogs.com/duanwandao/p/9922909.html)

ip138

接口 http://m,ip138.com/ip.asp?ip=ipaddress

交互式

```python
import requests
url = "http://m.ip138.com/ip.asp?ip="
try:
    r = requests.get(url+'202.204.80.112')
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    print(r.text[-500:])
except:
    print("抓取失败")
```



==以爬虫角度看待问题==

## 5.Beautiful Soup库入门

### 0x1 Beautiful Soup库的安装

对html解析，提取信息

https://www.crummy.com/software/BeautifulSoup/



####Beautiful Soup库的安装小测

https://python123.io/ws/demo.html

```html
<html><head><title>This is a python demo page</title></head>
<body>
<p class="title"><b>The demo python introduces several python courses.</b></p>
<p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:
<a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1">Basic Python</a> and <a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2">Advanced Python</a>.</p>
</body></html>
```

使用格式

<a href="https://imgchr.com/i/yEMM1U"><img src="https://s3.ax1x.com/2021/01/31/yEMM1U.png" alt="yEMM1U.png" border="0" /></a>

交互式代码

```pyhton
>>> import requests
>>> r = requests.get("https://python123.io/ws/demo.html")
>>> demo = r.text
>>> soup = BeautifulSoup(demo,'html.parser')
>>> print(soup.prettify())
<html>
 <head>
  <title>
   This is a python demo page
  </title>
 </head>
 <body>
  <p class="title">
   <b>
    The demo python introduces several python courses.
   </b>
  </p>
  <p class="course">
   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:
   <a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">
    Basic Python
   </a>
   and
   <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">
    Advanced Python
   </a>
   .
  </p>
 </body>
</html>
>>> 
```



####Beautiful Soup库的基本元素

Beautigul Soup库的理解

Beautigul Soup库是解析、遍历、维护“标签树”的功能库

针对html标签



####Beautigul Soup库的引用

```python
from bs4 import BeautifulSoup  #注意大写
import bs4
```



经过处理将html标签，转换为BeautifulSoup类

通过对BeautifulSoup类的

```python
#两种打开方式
from bs4 import BeautifulSoup  
soup = BeautifulSoup("<html>data</html>","html.parser")
soup2 = BeautifulSoup(open("D://demo.html"),"html.parser")
```



####Beautigul Soup库解析器

<a href="https://imgchr.com/i/yEJTQP"><img src="https://s3.ax1x.com/2021/01/31/yEJTQP.png" alt="yEJTQP.png" border="0" /></a>



####Beautigul Soup类的基本元素

<a href="https://imgchr.com/i/yEaDuq"><img src="https://s3.ax1x.com/2021/01/31/yEaDuq.png" alt="yEaDuq.png" border="0" /></a>



实例演示

标签现实查找

```Python
>>> from bs4 import BeautifulSoup
>>> demo = r.text
>>> soup = BeautifulSoup(demo,'html.parser')

>>> soup.head
<head><title>This is a python demo page</title></head>

>>> soup.a
<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>

>>> char = soup.p
>>> char
<p class="title"><b>The demo python introduces several python courses.</b></p>

>>> soup.a.parent.name
'p'
>>> soup.a.parent.parent.name
'body'
```



####标签属性查找

```python
>> tag = soup.a
>>> tag.attrs
{'href': 'http://www.icourse163.org/course/BIT-268001', 'class': ['py1'], 'id': 'link1'}
>>> tag.attrs['class']
['py1']
>>> tag.attrs['href']
'http://www.icourse163.org/course/BIT-268001'
>>> 
```



####html注释识别

因为soup会将所有子集元素识别，从而导致信息不是想要的信息

可以通过识别标签的类型来判断是否是注释

<a href="https://imgchr.com/i/yVPaB8"><img src="https://s3.ax1x.com/2021/01/31/yVPaB8.png" alt="yVPaB8.png" border="0" /></a>



####五种类型的对应关系

<a href="https://imgchr.com/i/yVPs9s"><img src="https://s3.ax1x.com/2021/01/31/yVPs9s.png" alt="yVPs9s.png" border="0" /></a>



###0x2 基于bs4库的HTML的遍历方法

根据标签的树形结构

有三种遍历方式

<a href="https://imgchr.com/i/yVip8A"><img src="https://s3.ax1x.com/2021/01/31/yVip8A.png" alt="yVip8A.png" border="0" /></a>



####标签树的下行遍历

<a href="https://imgchr.com/i/yViJ54"><img src="https://s3.ax1x.com/2021/01/31/yViJ54.png" alt="yViJ54.png" border="0" /></a>

```python
>>> import requests
>>> from bs4 import BeautifulSoup
>>> r = requests.get("https://python123.io/ws/demo.html")
>>> demo = r.text
>>> soup = BeautifulSoup(demo,'html.parser')

>>> soup.head
<head><title>This is a python demo page</title></head>

>>> soup.head.contents
[<title>This is a python demo page</title>]

>>> soup.body.contents
['\n', <p class="title"><b>The demo python introduces several python courses.</b></p>, '\n', <p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:
<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a> and <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>.</p>, '\n']

>>> len(soup.body.contents)
5

>>> soup.body.contents[1]   #可以用下标找出
<p class="title"><b>The demo python introduces several python courses.</b></p>
```

<a href="https://imgchr.com/i/yVEGO1"><img src="https://s3.ax1x.com/2021/01/31/yVEGO1.png" alt="yVEGO1.png" border="0" /></a>



####标签树的上行遍历

<a href="https://imgchr.com/i/yVENTK"><img src="https://s3.ax1x.com/2021/01/31/yVENTK.png" alt="yVENTK.png" border="0" /></a>

```python
>>> soup = BeautifulSoup(demo,'html.parser')
>>> soup.title.parent
<head><title>This is a python demo page</title></head>
>>> soup.html.parent
<html><head><title>This is a python demo page</title></head>
<body>
<p class="title"><b>The demo python introduces several python courses.</b></p>
<p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:

<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a> and <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>.</p>
</body></html>
>>> soup.parent
>>> 
```

```python
>>> import requests
>>> from bs4 import BeautifulSoup
>>> r = requests.get("https://python123.io/ws/demo.html")
>>> demo = r.text
>>> soup = BeautifulSoup(demo,'html.parser')
>>> for parent in soup.a.parents:
	if parent is None:
		print(parent)
	else:
		print(parent.name)

p
body
html
[document]
```



####标签树的平行遍历

<a href="https://imgchr.com/i/yV1GdJ"><img src="https://s3.ax1x.com/2021/02/01/yV1GdJ.png" alt="yV1GdJ.png" border="0" /></a>

<a href="https://imgchr.com/i/yV1ree"><img src="https://s3.ax1x.com/2021/02/01/yV1ree.png" alt="yV1ree.png" border="0" /></a>

```python
>>> soup = BeautifulSoup(demo,'html.parser')
>>> soup.a.next_sibling
' and '


>>> soup.a.next_sibling.next_sibling
<a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>
>>> soup.a.previous_sibling
'Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n'


>>> soup.a.previous_sibling.previous_sibling
>>> soup.a.parent
<p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:

<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a> and <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>.</p>
```

遍历

<a href="https://imgchr.com/i/yV1jyT"><img src="https://s3.ax1x.com/2021/02/01/yV1jyT.png" alt="yV1jyT.png" border="0" /></a>



### 0x3 基于bs4库的HTML格式输出

让HTML标签更友好的显示内容



####bs4的prettify()方法

为html标签文本增加换行符

也可以对单个标签操作

```pyhton
>>> import requests
>>> r = requests.get("https://python123.io/ws/demo.html")
>>> demo = r.text
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup(demo,'html.parser')
>>> soup.prettify()
'<html>\n <head>\n  <title>\n   This is a python demo page\n  </title>\n </head>\n <body>\n  <p class="title">\n   <b>\n    The demo python introduces several python courses.\n   </b>\n  </p>\n  <p class="course">\n   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n   <a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">\n    Basic Python\n   </a>\n   and\n   <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">\n    Advanced Python\n   </a>\n   .\n  </p>\n </body>\n</html>'
>>> print(soup.prettify())
<html>
 <head>
  <title>
   This is a python demo page
  </title>
 </head>
 <body>
  <p class="title">
   <b>
    The demo python introduces several python courses.
   </b>
  </p>
  <p class="course">
   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:
   <a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">
    Basic Python
   </a>
   and
   <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">
    Advanced Python
   </a>
   .
  </p>
 </body>
</html>
>>> 
```



##6.信息标记的三种形式

### 0x1 信息的标记

对不同信息进行区分，标记

标记后的信息可形成信息组织结构，增加了信息维度

标记后的信息可用于通信、存储或展示

标记的结构与信息一样具有重要价值

标记后的信息更有利于程序理解和运用



####HTML的信息标记

信息标记的三种形式 XML、json、YAML



####XML扩展标记语言

extensble markup language

```xml
<img src="china.jpg" size="10">...</img>

缩写为：
<img src="china.jpg" size="10"/>
注释格式与html相同
```

img 名称  src、size 属性  

与html接近





####json

javaScript Object Notation

对面向对象信息的表达形式

有类型键值对的表达形式

```json
"键key" : "值value”
"name" : "北京理工大学"
```

数字不需要加引号



可以包含多个

```json
"name" : ["北京理工大学"、"中南民族大学"] 
```



键值对可以嵌套

```json
"name" : {
    "newName" : "北京理工大学"，
    "oldName" ： "延安自然科学院"
         }
```

通过有类型的键值对整合数据



####YAML

YAML Ain‘t Markup Language

无类型键值对 key:value

```yaml
键key : 值value
name : 中南民族大学
```

通过缩进表示所属关系

```yaml
name:
  newName : 北京理工大学
  oldName : 延安自然科学院
```

用 - 号表达并列关系

```yaml
name:
-北京理工大学
-延安自然科学院
```

| 表达整块数据， #表示注释

```yaml
text: | #学校介绍
学校教育是由专职人员和专门机构承担的有目的、有系统、有组织的，有计划的以影响受教育学校教育者的身心发展为直接目标并最终使受教育者的身心发展达到预定目的的社会活动。学校教育指受教育者在各类学校内所接受的各种教育活动，是教育制度重要组成部分。学校教育的具体活动受到社会需求影响，必须符合社会发展趋势，承担着对社会输送人才的职能。一般说来，学校教育包括初等教育、中等教育和高等教育。学校主要分为五种：幼儿园、小学、初中、高中和大学。
```

###0x2 三种信息标记形式的比较





XML格式：通用语言，扩展好，繁琐

JSON：适合程序、信息有类型，比XML简洁

YAML：信息无类型，文本信息比例最高，可读性好



XML Internet上的交互与传递

JSON 程序的接口处理、移动应用云端和节点的信息通信，无注释

YAML 各类系统的配置文件，有注释易读



### 0x3 信息提取的一般方法

方法一：完整解析信息的标记形式，再提取关键信息

XML JSON YAML

需要标记解析器，例如标识库的标签树遍历



方法二：无视标记，直接搜索关键信息

简单，速度较快，但是结果不准确



####融合方法

结合解析与搜索，提取关键信息

思路：

搜索到所有同类标签

解析其标记形式，提取其信息



```python
>>> import requests
>>> r = requests.get("https://python123.io/ws/demo.html")
>>> demo = r.text
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup(demo,'html.parser')
>>> for link in soup.find_all('a'):
	print(link.get('href'))

	
http://www.icourse163.org/course/BIT-268001
http://www.icourse163.org/course/BIT-1001870001
```



### 0x4 基于bs4库的HTML内容查找方法



Beautiful Soup 提供的方法

```python
<>.find_all(name,attrs,recursive,string,**kwargs)
```

返回列表类型，存储查找结果



####name：

对标签名称的检索字符串

```python
>>> import requests
>>> from bs4 import BeautifulSoup
>>> r = requests.get("https://python123.io/ws/demo.html")
>>> demo = r.text
>>> soup = BeautifulSoup(demo,'html.parser')


>>> soup.find_all('a')
[<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>, <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>]


>>> soup.find_all(['a','b'])
[<b>The demo python introduces several python courses.</b>, <a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>, <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>]


>>> for tag in soup.find_all(True):
	print(tag.name)

	
html
head
title
body
p
b
p
a
a
>>> import re
>>> for tag in soup.find_all(re.compile('b')):
	print(tag.name)

	
body
b
```



####attrs：

对标签属性值的检索字符串，可标注属性检索

```python
>>> soup.find_all('p','course')
[<p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:

<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a> and <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>.</p>]


>>> soup.find_all(id='link1')
[<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>]


>>> soup.find_all(id='link')
[]


>>> import re
>>> soup.find_all(id=(re.compile('link')))
[<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>, <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>]
>>> 

```





####resursive:

是否对子孙全部检索，默认True

```python
>>> soup.find_all('a')
[<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>, <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>]


>>> soup.find_all('a',recursive=False)
[]
```



####string: 

对标签中间的字符串进行检索

```python
>>> soup.find_all(string = "Basic Python")
['Basic Python']

>>> import re
>>> soup.find_all(string=re.compile('Pyhton'))
[]

>>> soup.find_all(string=re.compile('Python'))
['Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n', 'Basic Python', 'Advanced Python']

>>> soup.find_all(string=re.compile('python'))
['This is a python demo page', 'The demo python introduces several python courses.']
```



####简写形式

```python
<tag>(..)  ==  <tag>.find_all(..)
soup(..)  ==  soup.find_all(..)
```



####扩展方法

<a href="https://imgchr.com/i/yVWLy6"><img src="https://s3.ax1x.com/2021/02/01/yVWLy6.png" alt="yVWLy6.png" border="0" /></a>



##7.实例 中国大学排名

https://www.shanghairanking.cn/rankings/bcur/2020



功能描述

输入：大学排名URL链接

输出：大学排名幸喜的屏幕输出（排名、大学名称、总分）

动态脚本信息不能获取

查看robots协议



获取内容

存到合适的数据结构

输出

采用二维列表



html内容分析

```html
<tbody data-v-2a8fd7e4>
    <tr data-v-2a8fd7e4>
        <td data-v-2a8fd7e4>1<!----></td>
        <td class="align-left" data-v-2a8fd7e4>
            <a href="/institution/tsinghua-university" data-v-2a8fd7e4>清华大学</a> 
            <p style="display:none" data-v-2a8fd7e4></p> 
        <!----></td>
        <td data-v-2a8fd7e4>北京<!----></td>
        <td data-v-2a8fd7e4>综合<!----></td>
        <td data-v-2a8fd7e4>852.5<!----></td>
        <td data-v-2a8fd7e4>38.2</td>
    </tr>
    
    。
    。
    。
</tbody>
```



定义函数

从网络上获取大学排名网页内容 

提取网页内容中信息到合适的数据结构

利用数据结构展示并输出

```python
import requests
import bs4
from bs4 import BeautifulSoup
#从网络上获取大学排名网页内容 
def getHTMLText(url): 
    try:
        r = requests.get(url,timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""

#提取网页内容中信息到合适的数据结构
def fillUnivList(ulist,html): 
    soup = BeautifulSoup(html,"html.parser")
    for tr in soup.find('tbody').children:  #找到tbody标签的子标签
        if isinstance(tr,bs4.element.Tag): #检测tr标签是否是bs4库定义的tag类型，不是就过滤，比如过滤注释
            a = tr('a')
            tds = tr('td') #将tr中的所有td标签存为列表赋值给tds
            ulist.append([tds[0].text.strip(), a[0].string.strip(), tds[4].text.strip()])


#利用数据结构展示并输出
def printUnivList(ulist,num):
    print("{:^10}\t{:^6}\t{:^10}".format("排名","学校名称","总分"))
    for i in range(num):
        u = ulist[i]
        print("{:^10}\t{:^6}\t{:^10}".format(u[0],u[1],u[2]))
    
#主函数
def main():
    uinfo = []
    url = 'https://www.shanghairanking.cn/rankings/bcur/202011'
    html = getHTMLText(url)
    fillUnivList(uinfo,html)
    printUnivList(uinfo,20) 
main()
```



优化

中文对齐问题的原因

当中文字符宽度不够时，采用西文字符填充；中西文字符占用宽度不同

解决方案

采用中文的空格填充 char（12288）

```python
import requests
import bs4
from bs4 import BeautifulSoup
#从网络上获取大学排名网页内容 
def getHTMLText(url): 
    try:
        r = requests.get(url,timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return ""

#提取网页内容中信息到合适的数据结构
def fillUnivList(ulist,html): 
    soup = BeautifulSoup(html,"html.parser")
    for tr in soup.find('tbody').children:  #找到tbody标签的子标签
        if isinstance(tr,bs4.element.Tag):#检测tr标签是否是bs4库定义的tag类型，不是就过滤，比如过滤注释
            a = tr('a')
            tds = tr('td') #将tr中的所有td标签存为列表赋值给tds
            ulist.append([tds[0].text.strip(), a[0].string.strip(), tds[4].text.strip()])
            #print(tds[0],tds[1],tds[4])
   


#利用数据结构展示并输出
def printUnivList(ulist,num):
    tplt = "{0:^10}{1:{3}^10}{2:^16}"
    print("{:^10}\t{:^6}\t{:^10}".format("排名","学校名称","总分"))
    for i in range(num):
        u = ulist[i]
        print(tplt.format(u[0],u[1],u[2],chr(12288)))
    
#主函数
def main():
    uinfo = []
    url = 'https://www.shanghairanking.cn/rankings/bcur/202011'
    html = getHTMLText(url)
    fillUnivList(uinfo,html)
    printUnivList(uinfo,20) 
main()


```

- 这里会有报错，原因是使用了string方法，而网页中有许多的空白string方法的作用是用来获取目标路径下第一个非标签字符串，得到的是个字符串(不知道的小伙伴现在也知道了哈)，但现在前面有很多空白，所以.string之后我们得到的也就是空白即NoneType。用text方法代替string方法即可。text的作用是用来获取目标路径下的子孙非标签字符串，返回的是字符串。