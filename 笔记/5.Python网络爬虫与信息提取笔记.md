#Python网络爬虫与信息提取笔记

掌握定向网络数据爬取和网页解析的基本能力

the website is the API



[toc]

##1.Python开发工具选择

IDE集成开发环境

<a href="https://imgchr.com/i/yFxQbR"><img src="https://s3.ax1x.com/2021/01/30/yFxQbR.png" alt="yFxQbR.png" border="0" /></a>



本课需要IDLE、PyCharm、Sublime Text、Anaconda & Spyder



IDLE python自带

Sublime Text 专为程序员

wing 公司维护工具收费，多人开发，版本同步

Visual Studio & PTVS  微软公司维护 win环境为主 调试丰富

Eclipse 通过PyDev配置 开源IDE开发工具

PyCharm 社区免费、简单、集成度高



科学计算领域

Canopy 公司维护，工具收费、支持500个第三方库

Anaconda 开源免费，更多第三方库



##2.Requests库入门

###0x1 requests的安装方法

www.python-requestes.org

```cmd
pip install requsets
```

测试

```python
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> r.status_code
200
>>> r.encoding = 'utf-8'
>>> r.text

输出
百度web页面的源码
```

安装正常



### 0x2 主要函数方法

<a href="https://imgchr.com/i/yk9gkn"><img src="https://s3.ax1x.com/2021/01/30/yk9gkn.png" alt="yk9gkn.png" border="0" /></a>



#### get方法

```python
r = requests.get(url)
```

构造一个向服务器请求资源的Request对象

返回一个包含服务器资源的Response对象

大小写敏感

```python
requests.get(url,params=None,**kwargs)
```

url 获取页面连接

params： url中的额外参数，字典或字节流格式，可选

**kwargs：12个控制访问的参数

<a href="https://imgchr.com/i/ykuE34"><img src="https://s3.ax1x.com/2021/01/30/ykuE34.png" alt="ykuE34.png" border="0" /></a>



####get包含两个对象

Request对象

Response对象 



####Response对象

```python
>>> import requests
>>> r = requests.get("http://www.baidu.com")  #访问网址
>>> print(r.status_code)  #请求页面的状态码
200   #状态码为200则访问成功，不是则失败
>>> type(r)  #检查r的类型
<class 'requests.models.Response'>  #输出了类名，是response
>>> r.headers
{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Sat, 30 Jan 2021 08:10:06 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:36 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}
```

Response对象属性

<a href="https://imgchr.com/i/yktqUJ"><img src="https://s3.ax1x.com/2021/01/30/yktqUJ.png" alt="yktqUJ.png" border="0" />



####获取网页流程

先检查状态，如果是200则可以解析

如果状态码是其他则，访问错误



####实例运用

```python
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> print(r.status_code)
200
>>> r.text
'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç\x99¾åº¦ä¸\x80ä¸\x8bï¼\x8cä½\xa0å°±ç\x9f¥é\x81\x93</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=ç\x99¾åº¦ä¸\x80ä¸\x8b class="bg s_btn"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ\x96°é\x97»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å\x9c°å\x9b¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§\x86é¢\x91</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å\x90§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç\x99»å½\x95</a> </noscript> <script>document.write(\'<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ \'" name="tj_login" class="lb">ç\x99»å½\x95</a>\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">æ\x9b´å¤\x9aäº§å\x93\x81</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å\x85³äº\x8eç\x99¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç\x94¨ç\x99¾åº¦å\x89\x8då¿\x85è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æ\x84\x8fè§\x81å\x8f\x8dé¦\x88</a>&nbsp;äº¬ICPè¯\x81030173å\x8f·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n'
>>> r.encoding
'ISO-8859-1'
>>> r.apparent_encoding
'utf-8'
>>> r.encoding = 'utf-8'
>>> r.text
'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=百度一下 class="bg s_btn"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write(\'<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ \'" name="tj_login" class="lb">登录</a>\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n'
>>> 
```

- 根据实例，如果网页服务器中含有charset字段，即说明服务器对自身资源编码有要求，访问时会获得该字段，并顺利解析出人类可读的内容

- 但如果没有charset字段，则会默认编码为'ISO-8859-1'，该编码不能解析中文，则需要用apparent_encoding分析其可解析为中文的编码方式。encoding只是在提取内容的编码，只有将apparent_encoding赋给encoding，才能获得中文可读的资源 



####理解Requests库的异常

<a href="https://imgchr.com/i/ykz4Kg"><img src="https://s3.ax1x.com/2021/01/30/ykz4Kg.png" alt="ykz4Kg.png" border="0" /></a>



判断http状态是否异常，异常返回Error

<a href="https://imgchr.com/i/yAPUXT"><img src="https://s3.ax1x.com/2021/01/30/yAPUXT.png" alt="yAPUXT.png" border="0" /></a>



####爬取网页的通用代码框架

使爬取网页更稳定有效

```python
import requests
def getHTMLText(url):
    try:
        r = requests.get(url,timeout=30)
        r.raise_for_status() #如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding  #使解码正确
        return r.text
    except:
        return"产生异常"
if _name_ == "_main_":
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```



####HTTP协议

超文本传输协议

HTTP是一个基于“请求与响应”模式的、无状态的应用层协议

采用URL格式 

```python
http：//host[:port][path]
```

host: 合法的Internet主机域名或IP地址

post：端口号，缺省端口为80

path：请求资源的路径



#####HTTP URL的理解：

URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源



####HTTP协议对资源的操作

<a href="https://imgchr.com/i/yAn4Mj"><img src="https://s3.ax1x.com/2021/01/30/yAn4Mj.png" alt="yAn4Mj.png" border="0" /></a>

<a href="https://imgchr.com/i/yAnxy9"><img src="https://s3.ax1x.com/2021/01/30/yAnxy9.png" alt="yAnxy9.png" border="0" /></a>

#####理解PATCH和PUT的区别

假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段。

需求：用户修改了UserName，其他不变

采用PATCH，仅向URL提交UserName的局部更新请求

采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除

PATCH节省网络带宽



####HTTP协议与Requests库的关系

是一一对应的

<a href="https://imgchr.com/i/yAuhtK"><img src="https://s3.ax1x.com/2021/01/31/yAuhtK.png" alt="yAuhtK.png" border="0" /></a>

requests.head方法

展示头部信息内容，用更少的网络流量

```python
>>> r = requests.head('http://httpbin.org/get')
>>> r.headers
{'Date': 'Sat, 30 Jan 2021 16:06:46 GMT', 'Content-Type': 'application/json', 'Content-Length': '307', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}
>>> r.text
''
```



requests.post方法

如果向URL POST一个字典、键值对，会默认添加到表单form的字段下

```python
>>> payload = {'key1':'value1','key2':'value2'}
>>> r = requests.post('http://httpbin.org/post',data = payload)
>>> print(r.text)
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "key1": "value1", 
    "key2": "value2"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "23", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.25.1", 
    "X-Amzn-Trace-Id": "Root=1-60158558-7cb778f10d734f1b605e2cae"
  }, 
  "json": null, 
  "origin": "121.56.130.130", 
  "url": "http://httpbin.org/post"
}
```

如果不post键值对，而是一个字符串，则会存储到data的字段内

```python
>>> r = requests.post('http://httpbin.org/post',data = 'ABC')
>>> print(r.text)
{
  "args": {}, 
  "data": "ABC", 
  "files": {}, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "3", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.25.1", 
    "X-Amzn-Trace-Id": "Root=1-60158634-3617046c013a64a120efe139"
  }, 
  "json": null, 
  "origin": "121.56.130.130", 
  "url": "http://httpbin.org/post"
}
```

###0x3主要方法的解析

####requests.request

```pyhton
requests.request(method,url,**kwargs)
```

######method:

请求方式，对应get/put/post等7种

<a href="https://imgchr.com/i/yAMtI0"><img src="https://s3.ax1x.com/2021/01/31/yAMtI0.png" alt="yAMtI0.png" border="0" /></a>

######**kwargs：

控制访问的参数，可选项，共13个

<a href="https://imgchr.com/i/yAM7dI"><img src="https://s3.ax1x.com/2021/01/31/yAM7dI.png" alt="yAM7dI.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQFYV"><img src="https://s3.ax1x.com/2021/01/31/yAQFYV.png" alt="yAQFYV.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQeOJ"><img src="https://s3.ax1x.com/2021/01/31/yAQeOJ.png" alt="yAQeOJ.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQKT1"><img src="https://s3.ax1x.com/2021/01/31/yAQKT1.png" alt="yAQKT1.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQlY6"><img src="https://s3.ax1x.com/2021/01/31/yAQlY6.png" alt="yAQlY6.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQ8SO"><img src="https://s3.ax1x.com/2021/01/31/yAQ8SO.png" alt="yAQ8SO.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQJ6e"><img src="https://s3.ax1x.com/2021/01/31/yAQJ6e.png" alt="yAQJ6e.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQYOH"><img src="https://s3.ax1x.com/2021/01/31/yAQYOH.png" alt="yAQYOH.png" border="0" /></a>

<a href="https://imgchr.com/i/yAQaTI"><img src="https://s3.ax1x.com/2021/01/31/yAQaTI.png" alt="yAQaTI.png" border="0" /></a>

url：获取页面的url链接

####requests.get

```python
requests.get(url,params=None,**kwargs)
```

url: 你获取页面的url链接

params：url中的额外参数，字典或字节流格式，可选

**kwargs：12个控制访问的参数，同上

####requests.head

```python
requests.head(url,**kwargs)
```

url: 你获取页面的url链接

**kwargs：13个控制访问的参数，同上

####requests.post

```python
requests.post(url,data=None.json=None,**kwargs)
```

url:拟更新页面的url链接

data：字典、字节序列或文件，Request的内容

json：JSON格式的数据，Request的内容

**kwargs：11个访问的参数，同上

####requests.put

```python
requests.put(url,data=None,**kwargs)
```

url:拟更新页面的url链接

data：字典、字节序列或文件，Request的内容

**kwargs：12个访问的参数，同上

####requests.patch

```python
requests.patch(url,data=None,**kwargs)
```

url:拟更新页面的url链接

data：字典、字节序列或文件，Request的内容

**kwargs：12个访问的参数，同上

####requests.delete

```python
requests.delete(url,**kwargs)
```

url:拟更新页面的url链接

**kwargs：13个访问的参数，同上



####爬取100次用时的代码

```python
def getHtmlText(url):
   try:  
       r = requests.get(url, timeout=30)  
       r.raise_for_status() 
       r.encoding = r.apparent_encoding        
       return r.text   
   except:
       return '运行异常'
if __name__ == "__main__": 
   url = 'https://www.baidu.com' #任意填入某个网址即可，我爬的百度
   totaltime = 0
   for i in range(100):
       starttime = time.perf_counter()
       getHtmlText(url)
       endtime = time.perf_counter()
       print('第{0}次爬取，用时{1:.4f}秒'.format(i+1, endtime-starttime))
       totaltime=totaltime+endtime-starttime       
   print('总共用时{:.4f}秒'.format(totaltime))
```



###0x2 关于http状态码

####1开头的http状态码

表示临时响应并需要请求者继续执行操作的状态代码。

100  （继续） 请求者应当继续提出请求。 服务器返回此代码表示已收到请求的第一部分，正在等待其余部分。 
101  （切换协议） 请求者已要求服务器切换协议，服务器已确认并准备切换。

####2开头的http状态码
表示请求成功

200   成功处理了请求，一般情况下都是返回此状态码； 
201   请求成功并且服务器创建了新的资源。 
202   接受请求但没创建资源； 
203   返回另一资源的请求； 
204   服务器成功处理了请求，但没有返回任何内容；
205   服务器成功处理了请求，但没有返回任何内容；
206   处理部分请求；

#### 3开头的http状态码（重定向） 

重定向代码，也是常见的代码

300  （多种选择） 针对请求，服务器可执行多种操作。 服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 
301  （永久移动） 请求的网页已永久移动到新位置。 服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。 
302  （临时移动） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 
303  （查看其他位置） 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。 
304  （未修改） 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。 
305  （使用代理） 请求者只能使用代理访问请求的网页。 如果服务器返回此响应，还表示请求者应使用代理。 
307  （临时重定向） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。


#### 4开头的http状态码表示请求出错

400   服务器不理解请求的语法。 
401  请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 
403  服务器拒绝请求。 
404  服务器找不到请求的网页。 
405  禁用请求中指定的方法。 
406  无法使用请求的内容特性响应请求的网页。 
407  此状态代码与 401类似，但指定请求者应当授权使用代理。 
408  服务器等候请求时发生超时。 
409  服务器在完成请求时发生冲突。 服务器必须在响应中包含有关冲突的信息。 
410  如果请求的资源已永久删除，服务器就会返回此响应。 
411  服务器不接受不含有效内容长度标头字段的请求。 
412  服务器未满足请求者在请求中设置的其中一个前提条件。 
413  服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 
414  请求的 URI（通常为网址）过长，服务器无法处理。 
415  请求的格式不受请求页面的支持。 
416  如果页面无法提供请求的范围，则服务器会返回此状态代码。 
417  服务器未满足”期望”请求标头字段的要求。

#### 5开头状态码并不常见

但是我们应该知道

500  （服务器内部错误） 服务器遇到错误，无法完成请求。 
501  （尚未实施） 服务器不具备完成请求的功能。 例如，服务器无法识别请求方法时可能会返回此代码。 
502  （错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。 
503  （服务不可用） 服务器目前无法使用（由于超载或停机维护）。 通常，这只是暂时状态。 
504  （网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 
505  （HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。

</a>

##3.robots.txt网络爬虫排除协议

网络爬虫引发的问题



网络爬虫的尺寸

<a href="https://imgchr.com/i/yAlqxS"><img src="https://s3.ax1x.com/2021/01/31/yAlqxS.png" alt="yAlqxS.png" border="0" /></a>



网络爬虫的骚扰

就如骚扰电话

服务器不能提供多次的访问



网络爬虫的法律风险

服务器上的数据又产权归属

网络爬虫获取数据后牟利是非法的



网络爬虫泄露隐私

网络爬虫可能具备突破简单访问控制的能力，

获取被保护数据从而泄露个人隐私



网络爬虫的限制

- 来源审查 判断User-Agent进行限制
  - 检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问

- 发布公告 Robots协议
  - 告知所有爬虫网址的爬取策略，要求爬虫遵守



Robots协议

网络爬虫排除标准

作用：网站告知所有爬虫网址的爬取策略

形式：在网站根目录下的robots.txt文件，写明了哪些网站不能爬取

<a href="https://imgchr.com/i/yA30t1"><img src="https://s3.ax1x.com/2021/01/31/yA30t1.png" alt="yA30t1.png" border="0" /></a>一一般网站都会有

用户名和不能访问的目录



Robots协议的遵守方式

网络爬虫：自动或人工识别robots.txt,再进行内容爬取

约束性：Robots协议是建议但非约束性，网络怕虫可以不遵守，但存在法律风险



类人行为可不参考Robots协议

##4.Requests库实战项目

